# Tarea Integradora 02: Detección de Fraude con Tarjetas de Crédito

**Dataset**: [Credit Card Fraud Detection (Kaggle)](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)

Este dataset contiene 284.807 transacciones realizadas con tarjeta de crédito en septiembre de 2013 por titulares europeos. De estas, 492 son fraudes (≈0.17%). El objetivo de esta tarea es aplicar técnicas de aprendizaje automático en un contexto realista y altamente desbalanceado.

---

## Objetivos de aprendizaje

- Comprender el impacto del desbalance de clases en problemas reales.
- Explorar y analizar un dataset financiero con fuerte desbalance.
- Aplicar técnicas de preprocesamiento y re-muestreo (undersampling, oversampling, SMOTE).
- Entrenar y comparar modelos de clasificación (Regresión Logística, Árboles de Decisión, Random Forest, Gradient Boosting).
- Evaluar modelos con métricas adecuadas para datasets desbalanceados (Precision, Recall, F1, AUC-ROC, AUC-PR).
- Reflexionar sobre los trade-offs entre precisión y recall en un problema crítico como el fraude.

---

## Consignas

1. **Carga y exploración inicial**
   - Descargar el dataset desde Kaggle.
   - Reportar el número de transacciones normales y fraudulentas.
   - Calcular y graficar la proporción de fraude.

2. **Análisis exploratorio**
   - Verificar si las variables requieren normalización o escalado.
   - Analizar la distribución de la variable `Amount` y el comportamiento de la variable `Time`.
   
3. **Modelo base**
   - Separar train/test usando `stratify=y`.
   - Entrenar un modelo de Regresión Logística sin re-balanceo.
   - Reportar métricas: Accuracy, Balanced Accuracy, Precision, Recall, F1, AUC-ROC, AUC-PR.

4. **Tratamiento del desbalance**
   - Aplicar al menos dos técnicas distintas:
     - Uso de `class_weight='balanced'` en modelos.
     - Re-muestreo con SMOTE (o variantes).
     - Undersampling de la clase mayoritaria.
   - Comparar los resultados de las métricas.

5. **Modelos avanzados**
   - Entrenar al menos un Árbol de Decisión y un Random Forest.
   - Ajustar hiperparámetros básicos (`max_depth`, `n_estimators`).
   - Comparar el rendimiento en términos de AUC-PR.

6. **Curvas comparativas**
   - Graficar la Curva ROC y la Curva Precision–Recall para los modelos principales.
   - Discutir cuál refleja mejor la dificultad del problema.

7. **Reflexión crítica**
   - Responder:
     - ¿Por qué accuracy es engañosa en este dataset?
     - ¿Qué métrica recomendarías para un sistema de detección de fraude en producción?
     - ¿Qué trade-off sería más importante: minimizar falsos positivos o falsos negativos?

---

## Entregables

- Un notebook en Python (Jupyter/Colab) con todo el análisis, código y visualizaciones.
- Un breve informe en Markdown o PDF (1–2 páginas) con:
  - Descripción del proceso seguido.
  - Tablas comparativas de métricas.
  - Reflexiones finales sobre el problema.

---

## Recomendaciones

- Usar `train_test_split(..., stratify=y)` para mantener la proporción de fraude.
- Utilizar la librería `imblearn` para aplicar SMOTE dentro de un pipeline.
- Reportar métricas con `classification_report` y funciones como `roc_auc_score` y `average_precision_score`.
- Evitar el data leakage: aplicar cualquier re-muestreo solo en el conjunto de entrenamiento, nunca antes de dividir en train/test.
