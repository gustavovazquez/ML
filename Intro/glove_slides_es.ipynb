{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe8b2251",
   "metadata": {},
   "source": [
    "# Diapositiva 1: ¿Qué es GloVe?\n",
    "\n",
    "**GloVe** (Global Vectors for Word Representation) es un algoritmo de aprendizaje no supervisado desarrollado en Stanford en 2014 para obtener representaciones vectoriales densas de palabras.\n",
    "\n",
    "GloVe aprende vectores de palabras que reflejan **relaciones semánticas lineales** en el espacio vectorial. Por ejemplo, en el espacio embebido:\n",
    "\n",
    "\\[\n",
    "\\text{vec}(\"rey\") - \\text{vec}(\"hombre\") + \\text{vec}(\"mujer\") \\approx \\text{vec}(\"reina\")\n",
    "\\]\n",
    "\n",
    "GloVe se basa en estadísticas globales de coocurrencia de palabras en grandes corpus como Wikipedia y Gigaword."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200f8a7",
   "metadata": {},
   "source": [
    "# Diapositiva 2: Matriz de coocurrencia\n",
    "\n",
    "GloVe comienza construyendo una matriz \\( X_{ij} \\) de coocurrencia donde cada celda representa cuántas veces la palabra \\( j \\) aparece en el contexto de la palabra \\( i \\).\n",
    "\n",
    "Ejemplo simplificado:\n",
    "\n",
    "|        | perro | gato | hueso | correr |\n",
    "|--------|-------|------|--------|--------|\n",
    "| **perro** |   0   |  5   |   9    |   2    |\n",
    "| **gato**  |   5   |  0   |   2    |   1    |\n",
    "| **hueso** |   9   |  2   |   0    |   0    |\n",
    "\n",
    "La ventana de contexto puede ser simétrica o asimétrica y afecta la construcción de esta matriz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a9c356",
   "metadata": {},
   "source": [
    "# Diapositiva 3: Función objetivo del modelo\n",
    "\n",
    "GloVe aprende vectores \\( w_i \\) y \\( \\tilde{w}_j \\) de palabras (palabra central y de contexto), junto con sesgos \\( b_i \\) y \\( \\tilde{b}_j \\), que satisfacen:\n",
    "\n",
    "\\[\n",
    "w_i^\\top \\cdot \\tilde{w}_j + b_i + \\tilde{b}_j \\approx \\log(X_{ij})\n",
    "\\]\n",
    "\n",
    "El modelo busca que el producto punto entre vectores se aproxime al logaritmo del número de coocurrencias entre las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293cf012",
   "metadata": {},
   "source": [
    "# Diapositiva 4: Función de pérdida\n",
    "\n",
    "La función de pérdida que GloVe minimiza es:\n",
    "\n",
    "\\[\n",
    "J = \\sum_{i,j=1}^{V} f(X_{ij}) \\left(w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\right)^2\n",
    "\\]\n",
    "\n",
    "Donde:\n",
    "- \\( f(x) \\) es una función de ponderación que controla la contribución de cada par de palabras.\n",
    "- \\( f(x) = (x / x_{max})^\\alpha \\) para \\( x < x_{max} \\), y \\( f(x) = 1 \\) para \\( x \\ge x_{max} \\)\n",
    "- Esto evita que las coocurrencias raras dominen el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926363e",
   "metadata": {},
   "source": [
    "# Diapositiva 5: Comparación con Word2Vec\n",
    "\n",
    "| Característica         | GloVe (Global Vectors)            | Word2Vec                      |\n",
    "|------------------------|-----------------------------------|-------------------------------|\n",
    "| Fundamento             | Conteo de coocurrencias globales  | Predicción basada en contexto |\n",
    "| Método de entrenamiento| Matriz precomputada + regresión   | Entrenamiento directo         |\n",
    "| Objetivo               | Aproximar \\( \\log(X_{ij}) \\)   | Maximizar prob. condicional   |\n",
    "| Ventajas               | Eficiencia, relaciones lineales   | Precisión contextual          |\n",
    "\n",
    "GloVe es más eficiente en corpus grandes donde ya se pueden contar las coocurrencias. Word2Vec se adapta mejor a flujos de datos dinámicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e887b9",
   "metadata": {},
   "source": [
    "# Diapositiva 6: Uso práctico de GloVe\n",
    "\n",
    "## Con spaCy (modelo mediano):\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"rey reina\")\n",
    "print(doc[0].vector)\n",
    "```\n",
    "\n",
    "## Con Gensim (archivo GloVe original convertido):\n",
    "```python\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"glove.6B.100d.txt\", binary=False, no_header=True)\n",
    "model.similarity(\"king\", \"queen\")\n",
    "```\n",
    "\n",
    "También puedes convertir archivos GloVe con `gensim.scripts.glove2word2vec`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f267089f",
   "metadata": {},
   "source": [
    "# Diapositiva 7: ¿Por qué usar GloVe?\n",
    "\n",
    "- Representaciones densas y bajas en dimensionalidad.\n",
    "- Preservan relaciones semánticas y sintácticas.\n",
    "- Eficientes para tareas de NLP como:\n",
    "  - Clasificación de texto\n",
    "  - Detección de similitud semántica\n",
    "  - Análisis de sentimientos\n",
    "- Fáciles de integrar como embeddings preentrenados.\n",
    "\n",
    "GloVe es una excelente opción para representar texto cuando el contexto dinámico no es esencial."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
